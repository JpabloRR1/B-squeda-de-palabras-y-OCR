# -*- coding: utf-8 -*-
"""Extraccion de informacion en pdf

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1In4PJqobyyDPIRvcCMiY5R4QiScddhXL
"""

import os
import re
import pandas as pd
from PyPDF2 import PdfReader

def normalizar_catalogo(texto):
    """Normaliza el texto convirti√©ndolo a min√∫sculas y eliminando caracteres especiales"""
    if pd.isna(texto):
        return ""
    texto = str(texto).lower()
    texto = re.sub(r'[-_]', ' ', texto)
    texto = re.sub(r'[^a-zA-Z0-9\s.,;:\"?!¬°¬ø√°√©√≠√≥√∫√±]', '', texto)
    return texto

def pluralizar(palabra):
    """
    Genera la forma plural simple de una palabra en espa√±ol. (Reincorporada)
    """
    palabra = palabra.lower()
    if palabra.endswith("s") and len(palabra) > 2:
        return palabra
    if palabra.endswith("z"):
        return palabra[:-1] + "ces"
    vocales = 'aeiou√°√©√≠√≥√∫'
    if palabra[-1] not in vocales:
        return palabra + 'es'
    else:
        return palabra + "s"

def cargar_clave(desde_carpeta, archivo_clave):
    """
    Carga y normaliza palabras clave desde un archivo CSV.
    CORRECCI√ìN: Asegura que la 'clave' se agregue a los metadatos.
    """
    ruta_archivo = os.path.join(desde_carpeta, archivo_clave)
    palabras_con_meta = []
    columna_req = ['Competencia', 'Directrices', 'clave', 'sub_idc']

    if archivo_clave.endswith('.csv'):
        try:
            df = pd.read_csv(ruta_archivo, encoding='utf-8')
            if not all(col in df.columns for col in columna_req):
                print(f"Error: Faltan columnas requeridas. Esperadas: {columna_req}, Encontradas: {df.columns.tolist()}")
                return []
            for _, row in df.iterrows():
                if pd.notna(row['clave']):
                    singular = str(row['clave']).strip()
                    meta_data = {
                        'competencia': str(row['Competencia']).strip(),
                        'Directrices': str(row['Directrices']).strip(),
                        'clave': singular,
                        'sub_idc': str(row['sub_idc']).strip()
                    }
                    palabras_con_meta.append(meta_data)
        except Exception as e:
            print(f"Error al leer CSV: {e}")
            return []
    else:
        print(f"Formato de archivo no soportado: {archivo_clave}")
        return []


    unica_llave = {tuple(sorted(d.items())) for d in palabras_con_meta}
    unica_palabras = [dict(t) for t in unica_llave]

    return unica_palabras


def procesar_pdf_lineamientos(ruta_pdf):
    """Extrae texto de PDFs de lineamientos, limpiando encabezados y pies de p√°gina."""
    try:
        reader = PdfReader(ruta_pdf)
        full_text = []
        encabezados = set()
        pies = set()
        frases_a_excluir = [
            "‚Äú HACIENDA | Mantenimiento y Operaci√≥n de Soluciones Tecnol√≥gicas | 00=>T ..",
            "_ Lineamientos Operativos para el Desarrollo, ..C HACINDA | Mantenimiento y Operaci√≥n de Soluciones Tecnol√≥gicas eeSAT _..",
            "C HACIENDA | Mantenimiento y Operaci√≥n de Soluciones Tecnol√≥gicas [ J",
            "C√≥digo del Documento:",
            "Versi√≥n:",
            "Fecha de Elaboraci√≥n:",
            "No. De P√°gina:",
            "_ Lineamientos Operativos para el Desarrollo, .. C HACINDA | Mantenimiento y Operaci√≥n de Soluciones Tecnol√≥gicas eeSAT _..",
            "‚Äú HACIENDA | Mantenimiento y Operaci√≥n de Soluciones Tecnol√≥gicas | 00=>",
            "C HACINDA | Mantenimiento y Operaci√≥n de Soluciones Tecnol√≥gicas eeSAT _..",
            "1 17/02/2021",
            "Directrices de Operaci√≥n en Materia de Seguridad de la Informaci√≥n del Servicio de",
            "Administraci√≥n Tributaria",
            "C√≥digo del Documento Versi√≥n Fecha de Elaboraci√≥n No. de P√°gina",
            "5 14/07/2023 55",
            " W// ‚Äî C√≥digodelDocumento ‚Äî Versi√≥n ‚Äî ‚Äî FechadeElaboraci√≥n ‚Äî Nodeb√°gina ‚Äî _- 3 14/07/2023 27 ‚Äî ",
            "‚Äî ‚Äî‚ÄîV¬°[ nny MOO ‚Äî ‚Äî B",
            "HACIENDA :: SAT",
            "‚Äî C√≥digo del Documento ‚Äî OVersi√≥n ‚Äî ‚Äî FechadeElaboreci√≥n ‚Äî ‚Äî NodeP√°gina ‚Äî e",
            "‚Äî C√≥digodelDocumento ‚Äî Versi√≥n ‚Äî ‚Äî FechadeElaboraci√≥n ‚Äî Nodeb√°gina ‚Äî _-"
            ]


        for i in range(0, len(reader.pages)):
            pagina = reader.pages[i]
            try:
                texto = pagina.extract_text()
                if texto:
                    lineas = texto.split('\n')
                    if len(lineas) > 3:
                        lineas = lineas[8:-8]
                    texto_filtrado = ' '.join(lineas).strip()
                    if texto_filtrado:
                        full_text.append(texto_filtrado)
            except Exception as e:
                print(f"Advertencia: No se pudo extraer texto de la p√°gina {i+1} de {os.path.basename(ruta_pdf)}: {e}")
                continue

        for pagina in reader.pages:
            texto = pagina.extract_text()
            if texto:
                lineas = texto.split('\n')
                lineas_filtradas = []
                for line in lineas:
                    line = line.strip()
                    if line and line not in encabezados and line not in pies and not any(frase in line for frase in frases_a_excluir):
                        lineas_filtradas.append(line)
                if lineas_filtradas:
                    full_text.append('\n'.join(lineas_filtradas))

        return '\n\n'.join(full_text)

    except Exception as e:
        print(f"Error al procesar lineamientos: {e}")
        return ""

def normalizar_indices_avanzada(texto):
    """
    Corrige √≠ndices mal formateados. (Se mantiene como en el c√≥digo original)
    """
    patron1 = re.compile(r'(\n|^)\s*(\d{1,2}\.\d{1,2})(\d{1,2}\.\d{1,2}\.?\s*)')
    texto = re.sub(patron1, r'\1\2.\3', texto)
    patron2 = re.compile(r'(\n|^)\s*(\d{1,2}\.\d{1,2}\.\d{1,2})(\d{1,2}\.?\s*)')
    texto = re.sub(patron2, r'\1\2.\3', texto)
    return texto

def buscar_lineamientos_con_palabra(lineamientos_texto, sub_idc, palabra_clave):
    """
    Busca lineamientos por √≠ndice y verifica si el texto del lineamiento
    contiene la palabra clave (singular o plural).
    """
    if not sub_idc or pd.isna(sub_idc):
        return None
    escaped_idc = re.escape(sub_idc).replace(r'\.', r'\.?')
    keyword_singular = normalizar_catalogo(palabra_clave)
    keyword_plural = normalizar_catalogo(pluralizar(palabra_clave))


    pattern = rf'(?:^|\n)\s*({escaped_idc}.*?)(?=\n\s*\d+\.|\Z)'
    matches = re.finditer(pattern, lineamientos_texto, re.DOTALL)

    for match in matches:
        full_lineamiento_text = match.group(1).strip()
        normalized_lineamiento = normalizar_catalogo(full_lineamiento_text)

        if re.search(r'\b' + re.escape(keyword_singular) + r'\b', normalized_lineamiento) or \
           re.search(r'\b' + re.escape(keyword_plural) + r'\b', normalized_lineamiento):

            return full_lineamiento_text

    return None


def buscar_lineamiento_por_indice(lineamientos_texto, sub_idc):
    """
    Busca y devuelve el texto de un lineamiento bas√°ndose solo en el sub-√≠ndice.
    """
    if not sub_idc or pd.isna(sub_idc):
        return None
    escaped_idc = re.escape(sub_idc).replace(r'\.', r'\.?')
    pattern = rf'(?:^|\n)\s*({escaped_idc}.*?)(?=\n\s*\d+\.|\Z)'
    match = re.search(pattern, lineamientos_texto, re.DOTALL)
    if match:
        return match.group(1).strip()
    return None

# ================================================================
# ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† BLOQUE PRINCIPAL DE EJECUCI√ìN (CORREGIDO)
# ================================================================

def ejecutar_extraccion_lineamientos_directrices(ruta_principal):
    """Funci√≥n principal para extraer Lineamientos y Directrices en dos hojas de Excel."""

    RUTA_PRINCIPAL = r'D:\Servicio Social Junio- Diciembre- 2025- JPRR\CODIGOS Y RESULTADOS\Directrices y lineamientos'
    catalog_dir = os.path.join(RUTA_PRINCIPAL, "1. Catalogo")
    catalog_name = "catalogodepurado.csv"
    output_dir = os.path.join(RUTA_PRINCIPAL, "3. Salida_Resultado_busqueda")
    lineamientos_path = os.path.join(RUTA_PRINCIPAL, "2. Documentos_Especificos", "Lineamientos Operativos para el Desarrollo Mantenimiento y Operacion de Soluciones Tecnologicas del SAT_2021.pdf")
    directrices_path= os.path.join(RUTA_PRINCIPAL, "2. Documentos_Especificos", "1Directrices de Operaci√≥n en Materia de Seguridad de la Informaci√≥n Aplicables a los Servidores P√∫blicos y Terceros del SAT_2023_OCR.pdf") # Renombrado para claridad

    print("\n=== CONFIGURACI√ìN DE AN√ÅLISIS DE LINEAMIENTOS Y DIRECTRICES ===\n")
    print(f"Carpeta Principal: {RUTA_PRINCIPAL}")
    print(f"Cat√°logo: {os.path.join(catalog_dir, catalog_name)}")
    print(f"Salida: {output_dir}")
    print(f"Lineamientos: {lineamientos_path}")
    print(f"Directrices (DSI): {directrices_path}")


    if not os.path.exists(catalog_dir) or not os.path.exists(lineamientos_path) or not os.path.exists(directrices_path):
        print("\nError: Una o m√°s rutas de Cat√°logo/Documentos Espec√≠ficos no existen.")
        return

    llaves = cargar_clave(catalog_dir, catalog_name)
    if not llaves:
        print("\nNo se cargaron palabras clave v√°lidas desde el cat√°logo.")
        return

    print("\nüìÑ Extrayendo texto y pre-procesando Lineamientos y Directrices...")

    directrices_text =  procesar_pdf_lineamientos(directrices_path)



    lineamiento_text_raw = procesar_pdf_lineamientos(lineamientos_path)
    lineamiento_text = normalizar_indices_avanzada(lineamiento_text_raw)


    print("‚úÖ Pre-procesamiento de documentos completado.")

    resultados_lineamientos= []
    resultados_directrices=  []

    for kw_data in llaves:
        competencia = kw_data["competencia"]
        directriz_col = kw_data["Directrices"]
        indice = kw_data["sub_idc"]
        palabra = kw_data["clave"]


        if competencia.startswith("DSI"):
            fuente_ext = directrices_text
            lista_destino = resultados_directrices
            tipo = "Directriz"
        else:
            fuente_ext = lineamiento_text
            lista_destino = resultados_lineamientos
            tipo = "Lineamiento"


        texto_completo = buscar_lineamientos_con_palabra(fuente_ext, indice, palabra)

        if texto_completo:
            lista_destino.append({
                'Competencia': competencia,
                'Directrices_Columna': directriz_col,
                'Indice': indice,
                'Palabra Clave': palabra,
                'Tipo': tipo,
                f'Texto Completo de la {tipo}': texto_completo
            })

        else:
            texto_solo_indice = buscar_lineamiento_por_indice(fuente_ext, indice)

            if texto_solo_indice:
                lista_destino.append ({
                    'Competencia': competencia,
                    'Directrices_Columna': directriz_col,
                    'Indice': indice,
                    'Palabra Clave': palabra,
                    'Tipo': tipo,
                    f'Texto Completo de la {tipo}': texto_solo_indice
                })

            else:

                lista_destino.append ({
                    'Competencia': competencia,
                    'Directrices_Columna': directriz_col,
                    'Indice': indice,
                    'Palabra Clave': palabra,
                    'Tipo': tipo,
                    f'Texto Completo de la {tipo}': f"‚ùå No se encontr√≥ el √≠ndice {indice} o la palabra clave '{palabra}' en las {tipo}."
                })


    if not resultados_lineamientos and not resultados_directrices:
        print("\nNo se pudo extraer ning√∫n Lineamiento o Directriz con las claves proporcionadas.")
        return


    def agrupar_resultados(resultados_lista, tipo):

        if not resultados_lista:
            return pd.DataFrame()

        df= pd.DataFrame(resultados_lista).drop_duplicates()

        columnas_agrupacion = [
            'Competencia',
            'Directrices_Columna',
            'Indice',
            'Tipo',
            f'Texto Completo de la {tipo}'
        ]

        df_agrupado = df.groupby(columnas_agrupacion, dropna=False).agg({
            'Palabra Clave': lambda x: ', '.join(x.astype(str).unique())}).reset_index()

        orden_columnas = [
            'Competencia',
            'Directrices_Columna',
            'Indice',
            'Palabra Clave',
            'Tipo',
            f'Texto Completo de la {tipo}'
        ]

        columnas_existentes= [col for col in orden_columnas if col in df_agrupado.columns]

        return df_agrupado[columnas_existentes]




    df_lineamientos_agrupado = agrupar_resultados(resultados_lineamientos, "Lineamiento")
    df_directrices_agrupado = agrupar_resultados(resultados_directrices, "Directriz")

    output_file = os.path.join(output_dir, "Resultados_Extraccion_Documentos_Oficiales.xlsx")
    os.makedirs(output_dir, exist_ok=True)

    try:
        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
            if not df_lineamientos_agrupado.empty:
                df_lineamientos_agrupado.to_excel(writer, sheet_name='LINEAMIENTOS', index=False)
            if not df_directrices_agrupado.empty:
                df_directrices_agrupado.to_excel(writer, sheet_name='DIRECTRICES', index=False)

        print(f"\n‚úÖ Extracci√≥n completada. Resultados guardados en: {output_file}")
    except Exception as e:
        print(f"\nError al guardar el archivo de Excel: {e}")

if __name__ == "__main__":
    ejecutar_extraccion_lineamientos_directrices(None)