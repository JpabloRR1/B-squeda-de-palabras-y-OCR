# -*- coding: utf-8 -*-
"""Busqueda de palabras

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E5fX8gV-nYxthilqxlOinAQeBbg0BNM7
"""

import os
import re
import pandas as pd
from PyPDF2 import PdfReader
import shutil
from collections import defaultdict


def normalizar_catalogo(texto):
    if pd.isna(texto):
        return ""
    texto = str(texto).lower()
    texto = re.sub(r'[-_]', ' ', texto)
    texto = re.sub(r'[^a-zA-Z0-9\s.,;:\"?!¬°¬ø√°√©√≠√≥√∫√±]', '', texto)
    return texto

def pluralizar(palabra):
    palabra = palabra.lower()
    if palabra.endswith("s") and len(palabra) > 2:
        return palabra
    if palabra.endswith("z"):
        return palabra[:-1] + "ces"
    vocales = 'aeiou√°√©√≠√≥√∫'
    if palabra[-1] not in vocales:
        return palabra + 'es'
    else:
        return palabra + "s"

def cargar_clave(desde_carpeta, archivo_clave):
    ruta_archivo = os.path.join(desde_carpeta, archivo_clave)
    palabras_con_meta = []
    columna_req = ['Competencia', 'Directrices', 'clave', 'sub_idc']

    if archivo_clave.endswith('.csv'):
        try:
            df = pd.read_csv(ruta_archivo, encoding='utf-8')
            if not all(col in df.columns for col in columna_req):
                print(f"Error: Faltan columnas requeridas. Esperadas: {columna_req}, Encontradas: {df.columns.tolist()}")
                return []
            for _, row in df.iterrows():
                if pd.notna(row['clave']):
                    singular = str(row['clave']).strip()
                    plural = pluralizar(singular)
                    meta_data = {
                        'competencia': str(row['Competencia']).strip(),
                        'Directrices': str(row['Directrices']).strip(),
                        'sub_idc': str(row['sub_idc']).strip()
                    }
                    palabras_con_meta.append({**meta_data, 'clave': singular})
                    palabras_con_meta.append({**meta_data, 'clave': plural})
        except Exception as e:
            print(f"Error al leer CSV: {e}")
            return []
    elif archivo_clave.endswith('.txt'):
        try:
            with open(ruta_archivo, 'r', encoding='utf-8') as f:
                for line in f:
                    singular = line.strip()
                    if singular:
                        plural = pluralizar(singular)
                        meta_data = {
                            'competencia': 'N/A',
                            'Directrices': 'N/A',
                            'sub_idc': 'N/A'
                        }
                        palabras_con_meta.append({**meta_data, 'clave': singular})
                        palabras_con_meta.append({**meta_data, 'clave': plural})
        except Exception as e:
            print(f"Error al leer TXT: {e}")
            return []
    else:
        print(f"Formato de archivo no soportado: {archivo_clave}")
        return []

    unica_llave = {tuple(sorted(d.items())) for d in palabras_con_meta}
    unica_palabras = [dict(t) for t in unica_llave]

    return unica_palabras

def procesar_pdf(ruta_pdf, directorio_salida):
    try:
        nombre_txt = os.path.basename(ruta_pdf).rsplit('.', 1)[0] + '.txt'
        ruta_salida = os.path.join(directorio_salida, nombre_txt)
        reader = PdfReader(ruta_pdf)
        texto_paginas = []

        for pagina in reader.pages:
            try:
                texto = pagina.extract_text()
                if texto:
                    limpiar_text = re.sub(r'\n{2,}', '\n\n', texto)
                    limpiar_text = re.sub(r'[^a-zA-Z0-9\s.,;:\"?!¬°¬ø√°√©√≠√≥√∫√±]', '', limpiar_text)
                    limpiar_text = re.sub(r'[^\S\n]+', ' ', limpiar_text).strip()

                    lineas = limpiar_text.split('\n')
                    if len(lineas) >= 3:
                        lineas = lineas[3:]
                    else:
                        lineas = []
                    if len(lineas) >= 3:
                        lineas = lineas[:-3]
                    else:
                        lineas = []

                    texto_filtrado_pagina = '\n'.join(lineas).strip()
                    if texto_filtrado_pagina:
                        texto_paginas.append(texto_filtrado_pagina)
            except Exception as e:
                print(f"Advertencia: No se pudo extraer texto de una p√°gina en {os.path.basename(ruta_pdf)}: {e}")
                continue

        if texto_paginas:
            with open(ruta_salida, 'w', encoding='utf-8') as f:
                f.write('\n\n'.join(texto_paginas))
            return True
        return False

    except Exception as e:
        print(f"‚ùå Error al procesar PDF {os.path.basename(ruta_pdf)}: {str(e)}")
        return False


def procesar_pdf_lineamientos(ruta_pdf):
    try:
        reader = PdfReader(ruta_pdf)
        full_text = []


        patron_fecha = re.compile(r'^\s*\d{2}/\d{2}/\d{4}\s*$', re.IGNORECASE)
        patron_pie_variable = re.compile(
            r'^\s*\d+\s+de\s+\d+\s+Lineamientos\s+Operativos\s+para\s+el\s+Desarrollo.*$',
            re.IGNORECASE | re.DOTALL
        )

        patron_cabecera_tabla = re.compile(
            r'^\s*[\W\s]*C√≥digo[\W\s]+del[\W\s]+Documento[\W\s]+Versi√≥n[\W\s]+Fecha[\W\s]+de[\W\s]+Elaboraci√≥n[\W\s]+No\.\w*de[\W\s]+P√°gina[\W\s]*$',
            re.IGNORECASE | re.DOTALL
        )

        patron_valores_tabla = re.compile(
            r'^\s*[\W\s]*\d+[\W\s]*\d{1,2}/\d{1,2}/\d{4}[\W\s]*\d+[\W\s]*$',
            re.IGNORECASE
        )

        patron_texto_atipico_multi = re.compile(
            r'^\s*(?:X\'/|IZ√Å\s*D\s*E‚Äî\s*E\s*AE\s*e\s*u\s*X|w/ !|‚Äî\s*‚Äî‚ÄîV¬°\[\s*nny\s*MOO\s*‚Äî\s*‚Äî)\s*$',
            re.IGNORECASE
        )
        patron_sat = re.compile(
            r'^\s*HACIENDA[\W\s]*::[\W\s]*SAT[\W\s]*$|^\s*HACIENDA[\W\s]*S%√≠sar[\W\s]*$|^\s*HACIENDA[\W\s]*√≥gsar[\W\s]*$',
            re.IGNORECASE
        )

        patron_titulo_documento = re.compile(
            r'Directrices\W*de\W*Operaci√≥n\W*en\W*Materia\W*de\W*Seguridad\W*de\W*la\W*Informaci√≥n\W*del\W*Servicio\W*de\W*Administraci√≥n\W*Tributaria[\W\s]*$',
            re.IGNORECASE | re.DOTALL
        )
        patron_ruido_corto = re.compile(r'^\s*[\W\s]*[a-zA-Z0-9]{0,2}[\W\s]*$', re.IGNORECASE)


        frases_a_excluir = [
            "‚Äú HACIENDA | Mantenimiento y Operaci√≥n de Soluciones Tecnol√≥gicas | 00=>T ..",
            "_ Lineamientos Operativos para el Desarrollo, ..C HACINDA | Mantenimiento y Operaci√≥n de Soluciones Tecnol√≥gicas eeSAT _..",
            "C HACIENDA | Mantenimiento y Operaci√≥n de Soluciones Tecnol√≥gicas [ J",
            "C√≥digo del Documento:", "Versi√≥n:", "Fecha de Elaboraci√≥n:", "No. De P√°gina:",
            "C HACINDA | Mantenimiento y Operaci√≥n de Soluciones Tecnol√≥gicas eeSAT _..",
            "‚Äú HACIENDA | Mantenimiento y Operaci√≥n de Soluciones Tecnol√≥gicas | 00=>"
        ]

        for pagina in reader.pages:
            texto = pagina.extract_text()
            if texto:
                lineas = texto.split('\n')
                lineas_filtradas = []
                for line in lineas:
                    line = line.strip()
                    if not line:
                        continue

                    es_encabezado_o_pie_variable = (
                        patron_fecha.search(line) or
                        patron_pie_variable.search(line) or
                        patron_cabecera_tabla.search(line) or
                        patron_valores_tabla.search(line) or
                        patron_texto_atipico_multi.search(line) or
                        patron_sat.search(line) or
                        patron_titulo_documento.search(line) or
                        patron_ruido_corto.search(line)
                    )

                    es_frase_exacta = any(frase in line for frase in frases_a_excluir)

                    if not es_encabezado_o_pie_variable and not es_frase_exacta:
                        lineas_filtradas.append(line)

                if lineas_filtradas:
                    lineas_a_unir = lineas_filtradas[4:-4]

                    if lineas_a_unir:
                        full_text.append('\n'.join(lineas_a_unir))

        return '\n\n'.join(full_text)

    except Exception as e:
        print(f"Error al procesar lineamientos: {e}")
        return ""


def normalizar_indices_avanzada(texto):
    patron1 = re.compile(r'(\n|^)\s*(\d{1,2}\.\d{1,2})(\d{1,2}\.\d{1,2}\.?\s*)')
    texto = re.sub(patron1, r'\1\2.\3', texto)
    patron2 = re.compile(r'(\n|^)\s*(\d{1,2}\.\d{1,2}\.\d{1,2})(\d{1,2}\.?\s*)')
    texto = re.sub(patron2, r'\1\2.\3', texto)
    return texto

def buscar_lineamientos_con_palabra(lineamientos_texto, sub_idc, palabra_clave):
    if not sub_idc or pd.isna(sub_idc):
        return None
    escaped_idc = re.escape(sub_idc).replace(r'\.', r'\.?')
    keyword_singular = normalizar_catalogo(palabra_clave)
    keyword_plural = normalizar_catalogo(pluralizar(palabra_clave))
    pattern = rf'(?:^|\n)\s*({escaped_idc}.*?)(?=\n\s*\d+\.|\Z)'
    matches = re.finditer(pattern, lineamientos_texto, re.DOTALL | re.IGNORECASE)

    for match in matches:
        full_lineamiento_text = match.group(1).strip()
        if re.search(r'\b' + re.escape(keyword_singular) + r'\b', full_lineamiento_text, re.IGNORECASE) or \
           re.search(r'\b' + re.escape(keyword_plural) + r'\b', full_lineamiento_text, re.IGNORECASE):
            texto_sin_indice = re.sub(rf'^{re.escape(sub_idc)}\s*', '', full_lineamiento_text, 1, flags=re.IGNORECASE).strip()
            return texto_sin_indice

    return None

def buscar_lineamiento_por_indice(lineamientos_texto, sub_idc):
    if not sub_idc or pd.isna(sub_idc):
        return None
    escaped_idc = re.escape(sub_idc).replace(r'\.', r'\.?')
    pattern = rf'(?:^|\n)\s*({escaped_idc}.*?)(?=\n\s*\d+\.|\Z)'
    match = re.search(pattern, lineamientos_texto, re.DOTALL)
    if match:
        full_lineamiento_text = match.group(1).strip()
        texto_sin_indice = re.sub(rf'^{re.escape(sub_idc)}\s*', '', full_lineamiento_text, 1).strip()
        return texto_sin_indice
    return None

def buscar_en_archivo(ruta_archivo, palabras_clave_busqueda):
    coincidencias = []
    try:
        with open(ruta_archivo, 'r', encoding='utf-8') as f:
            contenido = f.read()
        paragraphs = [p.strip() for p in re.split(r'\n\s*\n', contenido) if p.strip()]
        for keyword_data in palabras_clave_busqueda:
            normalized_keyword = normalizar_catalogo(keyword_data['clave'])
            pattern = re.compile(r'\b' + re.escape(normalized_keyword) + r'\b', re.IGNORECASE)
            for para in paragraphs:
                if pattern.search(normalizar_catalogo(para)):
                    coincidencias.append({
                        'Competencia': keyword_data['competencia'],
                        'Directrices': keyword_data['Directrices'],
                        'Indice': keyword_data['sub_idc'],
                        'Palabra': keyword_data['clave'],
                        'P√°rrafo': para.replace('\n', ' ').strip()
                    })
    except Exception as e:
        print(f"Error al procesar {ruta_archivo}: {e}")
    return coincidencias



def procesar_acuerdo_articulos(ruta_pdf, articulos_permitidos= None):
    articulos_encontrados = []
    try:
        reader = PdfReader(ruta_pdf)
        texto_completo = ""
        encabezados = set()
        pies = set()
        if len(reader.pages) > 2:
            for i in range(2, min(len(reader.pages), 5)):
                pagina = reader.pages[i].extract_text()
                if pagina:
                    lineas = pagina.split('\n')
                    if len(lineas) > 3:
                        encabezados.add(lineas[0].strip())
                        pies.add(lineas[-1].strip())
        texto_limpio_paginas = []
        for page in reader.pages:
            texto_pagina = page.extract_text()
            if texto_pagina:
                lineas = texto_pagina.split('\n')
                lineas_limpias = [
                    line.strip() for line in lineas
                    if line.strip() and line.strip() not in encabezados and line.strip() not in pies
                ]
                texto_limpio_paginas.append(" ".join(lineas_limpias))
        texto_completo = " ".join(texto_limpio_paginas)
        texto_completo = re.sub(r'\s+', ' ', texto_completo).strip()
        patron_separador = re.compile(
            r'(Art√≠culo\s+\d+\.-.*?)(?=(?:Art√≠culo\s+\d+\.-|\Z))',
            re.IGNORECASE | re.DOTALL
        )
        secciones_split = patron_separador.finditer(texto_completo)
        for match in secciones_split:
            articulo_completo = match.group(1).strip()
            titulo_match = re.search(r'Art√≠culo\s+\d+\.-', articulo_completo, re.IGNORECASE)
            if titulo_match:
                titulo = titulo_match.group(0).strip()
                if articulos_permitidos is not None:
                    titulo_norm = titulo.lower().replace(' ', '')
                    if titulo_norm not in [a.lower().replace(' ', '') for a in articulos_permitidos]:
                        continue

                contenido = articulo_completo.replace(titulo, '', 1).strip()
                contenido = re.sub(r'DIARIO OFICIAL.*?|Lunes.*?|T√çTULO.*?|CAP√çTULO.*?', '', contenido, flags=re.I|re.DOTALL)
                contenido = re.sub(r'\s+', ' ', contenido).strip()

                articulos_encontrados.append({
                    'articulo': titulo,
                    'contenido': contenido
                })
        return articulos_encontrados
    except Exception as e:
        print(f"‚ùå Error al procesar el PDF de acuerdo: {e}")
        return []

def buscar_articulo_por_palabra(acuerdo_data, palabra):
    encontrados = []
    normalized_keyword = normalizar_catalogo(palabra)
    pattern = re.compile(r'\b' + re.escape(normalized_keyword) + r'\b', re.IGNORECASE)
    for articulo_data in acuerdo_data:
        if pattern.search(normalizar_catalogo(articulo_data['contenido'])):
            encontrados.append({
                'articulo': articulo_data['articulo'],
                'contenido': articulo_data['contenido']
            })
    if encontrados:
        numeros = ", ".join(sorted([a['articulo'] for a in encontrados]))
        return {'numeros': numeros, 'articulos_completos': encontrados}
    else:
        return None

def generar_ranking_subindices(df_resultados):
    df_conteo = df_resultados[['Archivo', 'Competencia', 'SubIndice']].copy()

    def clasificar_tipo(competencia):
        return 'Directriz' if str(competencia).strip().upper().startswith("DSI") else 'Lineamiento'

    df_conteo['Tipo'] = df_conteo['Competencia'].apply(clasificar_tipo)

    conteo_subindices = (
        df_conteo.groupby(['Archivo', 'Tipo', 'SubIndice'])
        .size()
        .reset_index(name='Cuenta')
    )

    ranking_final = []
    subindices_frecuentes = set()

    for archivo in conteo_subindices['Archivo'].unique():
        ranking_archivo = {'Archivo': archivo}

        df_archivo = conteo_subindices[conteo_subindices['Archivo'] == archivo]

        for tipo in ['Lineamiento', 'Directriz']:
            df_tipo = df_archivo[df_archivo['Tipo'] == tipo]

            top_10 = df_tipo.sort_values(by=['Cuenta', 'SubIndice'], ascending=[False, True]).head(5)

            for sub_idc in top_10['SubIndice']:
                subindices_frecuentes.add(sub_idc)

            lista_top_10 = [
                f"{sub_idc} ({cuenta})"
                for sub_idc, cuenta in zip(top_10['SubIndice'], top_10['Cuenta'])
            ]

            columna_nombre = f'SubIndices mas frecuentes de {tipo} (Indice: Conteo)'
            ranking_archivo[columna_nombre] = "\n".join(lista_top_10) if lista_top_10 else "N/A"

        ranking_final.append(ranking_archivo)

    return pd.DataFrame(ranking_final), subindices_frecuentes


def generar_hoja_subindices_frecuentes(df_resultados_completo, subindices_frecuentes):
    """
    Filtra el DataFrame completo para incluir solo las filas
    cuyo SubIndice se encuentra entre los m√°s frecuentes.
    """
    if not subindices_frecuentes:
        print("Advertencia: El conjunto de sub√≠ndices frecuentes est√° vac√≠o.")
        return pd.DataFrame()

    df_filtrado = df_resultados_completo[
        df_resultados_completo['SubIndice'].isin(subindices_frecuentes)
    ].copy()

    df_filtrado = df_filtrado.sort_values(by=['Archivo', 'Competencia', 'SubIndice'])

    return df_filtrado




RUTA_PRINCIPAL = r'D:\Servicio Social Junio- Diciembre- 2025- JPRR\CODIGOS Y RESULTADOS\Busqueda de palabras'

input_dir = os.path.join(RUTA_PRINCIPAL, "1. Entrada_Archivos_Busqueda")
catalog_dir = os.path.join(RUTA_PRINCIPAL, "2. Catalogo")
catalog_name_2 = "Directrices.csv"
catalog_name_3 = "Lineamientos.csv"
catalogo_completo= "catalogodepurado.csv"


while True:

    entrada = input("¬øIngresa el numero del catalogo deseado? \n 1.Lineamientos \n 2. Directrices \n 3. Ambos      ")
    print("")

    try:

        catalogo_usar = int(entrada)

        if catalogo_usar == 1:
            catalogo_usar = catalog_name_3
            break
        elif catalogo_usar == 2:
            catalogo_usar = catalog_name_2
            break
        elif catalogo_usar == 3:
            catalogo_usar= catalogo_completo
            break

        else:
            print("")
            print("üö® Opci√≥n no v√°lida. Por favor, ingresa 1, 2 o 3.")
            print("")

    except ValueError:

        print("‚ùå Entrada inv√°lida. Por favor, ingresa un valor num√©rico.")


output_dir = os.path.join(RUTA_PRINCIPAL, "4. Salida_Resultado_busqueda")
lineamientos_path = os.path.join(RUTA_PRINCIPAL, "3. Documentos_Especificos", "Lineamientos Operativos para el Desarrollo Mantenimiento y Operacion de Soluciones Tecnologicas del SAT_2021.pdf")
acuerdo_path = os.path.join(RUTA_PRINCIPAL, "3. Documentos_Especificos", "ACUERDO por el que se emiten las pol√≠ticas y disposiciones para impulsar el uso y aprovechamiento de la inform√°tica (003).pdf")
directrices= os.path.join(RUTA_PRINCIPAL, "3. Documentos_Especificos", "1Directrices de Operaci√≥n en Materia de Seguridad de la Informaci√≥n Aplicables a los Servidores P√∫blicos y Terceros del SAT_2023_OCR.pdf")

print("\n=== CONFIGURACI√ìN DE AN√ÅLISIS DE ARCHIVOS (RUTAS FIJAS) ===\n")
print(f"Carpeta Principal: {RUTA_PRINCIPAL}")
print(f"Archivos de Entrada: {input_dir}")
print(f"Cat√°logo: {os.path.join(catalog_dir, catalogo_usar)}")
print(f"Salida: {output_dir}")
print(f"Lineamientos: {lineamientos_path}")
print(f"Acuerdo: {acuerdo_path}")

if not all(os.path.exists(d) for d in [input_dir, catalog_dir]) or \
   not os.path.exists(lineamientos_path) or not os.path.exists(acuerdo_path):
    print("\nError: Una o m√°s rutas especificadas no existen. Verifica la 'RUTA_PRINCIPAL' y las subcarpetas.")
    exit()

llaves = cargar_clave(catalog_dir, catalogo_usar)
if not llaves:
    print("\nNo se cargaron palabras clave v√°lidas desde el cat√°logo")
    exit()

while True:
    usar_filtro= input("¬øDeseas utilizar el filtro de b√∫squeda de palabra en el t√≠tulo? (si/no): ").lower().strip()
    if usar_filtro in  ['si', 's', 's√≠']:
        usar_filtro = True
        print("‚úÖ Usando filtro de t√≠tulo para pre-seleccionar palabras clave.")
        break

    elif usar_filtro in ['no', 'n']:
        usar_filtro = False
        break

    else:
        print("Respuesta no v√°lida. Por favor, ingresa 'si' o 'no'.")

print("\nüîç Realizando pre-filtrado de palabras clave por t√≠tulo...")


keywords_by_file = defaultdict(list)
archivos_a_procesar = {}
for root, _, files in os.walk(input_dir):
    for file in files:
        if file.startswith('.') or file.endswith('~'):
            continue

        full_path = os.path.join(root, file)
        archivos_a_procesar[file] = full_path

        if usar_filtro:
            file_name_normalized = normalizar_catalogo(os.path.splitext(file)[0])
            for keyword_data in llaves:
                normalizar_llaves = normalizar_catalogo(keyword_data['clave'])

                patterns = []
                if len(normalizar_llaves) > 2:
                    if normalizar_llaves.endswith('s'):
                        singular = normalizar_llaves[:-1]
                        if singular.endswith('e'):
                            singular = normalizar_llaves[:-2]
                        patterns.append(re.compile(r'\b' + re.escape(singular) + r'\b', re.IGNORECASE))

                    patterns.append(re.compile(r'\b' + re.escape(normalizar_llaves) + r'\b', re.IGNORECASE))
                patterns = list(set(patterns))

                for pattern in patterns:
                    if pattern.search(file_name_normalized):
                        keywords_by_file[file].append(keyword_data)
        else:

            keywords_by_file[file] = llaves

if not keywords_by_file:
    print("‚ö†Ô∏è No se encontraron palabras clave en los t√≠tulos de ning√∫n archivo. El programa terminar√°.")
    exit()


if usar_filtro:

    if not any(keywords_by_file.values()):
        print("‚ö†Ô∏è No se encontraron palabras clave en los t√≠tulos de ning√∫n archivo. El programa terminar√°.")
        exit()
    print(f"‚úÖ Se encontraron coincidencias en los t√≠tulos de {len(keywords_by_file)} archivos.")
else:

    print(f"‚úÖ Se procesar√°n {len(archivos_a_procesar)} archivos sin filtro por t√≠tulo.")


print("‚è≥ Procesando Directrices (puede tardar)...")
directrices_text= procesar_pdf_lineamientos(directrices)
print("‚úÖ Directrices procesadas.")

print("‚è≥ Procesando Lineamientos Operativos (puede tardar)...")
lineamiento_text_raw = procesar_pdf_lineamientos(lineamientos_path)
lineamiento_text = normalizar_indices_avanzada(lineamiento_text_raw)
print("‚úÖ Lineamientos Operativos procesados.")

articulosExtraer= ["Art√≠culo 1.-", "Art√≠culo 2.-", "Art√≠culo 45.-", "Art√≠culo 46.-", "Art√≠culo 50.-", "Art√≠culo 51.-", "Art√≠culo 52.-", "Art√≠culo 53.-",
                    "Art√≠culo 54.-", "Art√≠culo 55.-", "Art√≠culo 56.-", "Art√≠culo 58.-", "Art√≠culo 59.-",
                    "Art√≠culo 60.-", "Art√≠culo 61.-", "Art√≠culo 62.-", "Art√≠culo 63.-", "Art√≠culo 64.-",
                    "Art√≠culo 65.-", "Art√≠culo 66.-", "Art√≠culo 67.-", "Art√≠culo 68.-" , "Art√≠culo 69.-",
                    "Art√≠culo 70.-", "Art√≠culo 71.-", "Art√≠culo 73.-", "Art√≠culo 74.-", "Art√≠culo 76.-",
                    "Art√≠culo 87.-", "Art√≠culo 88.-"
                    ]

print("üìú Pre-procesando 'acuerdo.pdf' para b√∫squeda r√°pida...")
acuerdo_data = procesar_acuerdo_articulos(acuerdo_path, articulos_permitidos= articulosExtraer)
print("‚úÖ 'acuerdo.pdf' procesado.")

txt_dir = os.path.join(output_dir, "Archivos_Texto")
os.makedirs(txt_dir, exist_ok=True)
processed_files = 0


grouped_resultados = defaultdict(lambda: {
    'Palabras Clave': set(),
    'P√°rrafos Encontrados': set(),
    'Archivo': '',
    'Lineamientos': 'No aplica (DSI)',
    'Articulos del Acuerdo': set()
})

articulos_palabras_clave = defaultdict(set)


for original_file, claves_en_titulo in keywords_by_file.items():
    full_path = archivos_a_procesar[original_file]

    if os.path.isfile(full_path):
        txt_filename = os.path.splitext(original_file)[0] + '.txt'
        txt_filepath = os.path.join(txt_dir, txt_filename)


        if original_file.lower().endswith('.pdf'):
            if not procesar_pdf(full_path, txt_dir):
                continue
        elif original_file.lower().endswith('.txt'):
            try:
                shutil.copy2(full_path, txt_filepath)
            except Exception as e:
                print(f"Error al copiar {original_file}: {e}")
                continue
        else:
            continue

        processed_files += 1
        grupos_relevantes = set()
        for kw in claves_en_titulo:
            grupos_relevantes.add((kw['competencia']))
        buscar_llaves_entxt = [
            kw_from_catalog
            for kw_from_catalog in llaves
            if (kw_from_catalog['competencia']) in grupos_relevantes
        ]

        coincidencias_en_texto = buscar_en_archivo(txt_filepath, buscar_llaves_entxt)

        if coincidencias_en_texto:
            for match in coincidencias_en_texto:
                key = (
                    match['Competencia'],
                    match['Directrices'],
                    match['Indice'],
                    match['P√°rrafo']
                )

                grouped_resultados[key]['Archivo'] = original_file
                grouped_resultados[key]['Palabras Clave'].add(match['Palabra'])
                grouped_resultados[key]['P√°rrafos Encontrados'].add(match['P√°rrafo'])

                lineamiento = "No se encontr√≥ el Lineamiento/Directriz."


                if match['Competencia'].startswith("DSI"):
                    fuente_ext = directrices_text
                    lineamiento_tipo = "Directrices"
                else:
                    fuente_ext = lineamiento_text
                    lineamiento_tipo = "Lineamientos"

                lineamiento = "No se encontr√≥ el Lineamiento/Directriz."


                lineamiento_encontrado = buscar_lineamientos_con_palabra(
                    fuente_ext,
                    match['Indice'],
                    match['Palabra']
                )

                if lineamiento_encontrado is None:
                    lineamiento_encontrado = buscar_lineamiento_por_indice(
                        fuente_ext,
                        match['Indice']
                    )


                if lineamiento_encontrado is None:
                    lineamiento = f"No se encontr√≥ el √≠ndice o la palabra clave en las {lineamiento_tipo}."
                else:
                    lineamiento = lineamiento_encontrado

                grouped_resultados[key]['Lineamientos'] = lineamiento


                resultados_acuerdo = buscar_articulo_por_palabra(acuerdo_data, match['Palabra'])
                if resultados_acuerdo:
                    grouped_resultados[key]['Articulos del Acuerdo'].update(resultados_acuerdo['numeros'].split(', '))
                    for art_data in resultados_acuerdo['articulos_completos']:
                        articulos_palabras_clave[(original_file, art_data['articulo'])].add(match['Palabra'])

articulos_agrupados = {}
for key, palabras_encontradas in articulos_palabras_clave.items():
    original_file, articulo_titulo = key
    articulo_info = next((item for item in acuerdo_data if item["articulo"] == articulo_titulo), None)
    if articulo_info:
        contenido_resaltado = articulo_info["contenido"]
        for palabra in palabras_encontradas:
            regex = re.compile(rf'(\b{re.escape(palabra)}\b)', re.IGNORECASE)
            contenido_resaltado = regex.sub(r'**\1**', contenido_resaltado)
        articulos_agrupados[key] = {
            'Archivo': original_file,
            'Palabras Clave': palabras_encontradas,
            'Articulo Completo': f"{articulo_info['articulo']}: {contenido_resaltado}"
        }

if processed_files == 0:
    print("\nNo se encontraron archivos PDF o TXT v√°lidos para procesar")
    exit()
if not grouped_resultados:
    print("\nNo se encontraron coincidencias de palabras clave en el contenido de ning√∫n archivo.")
else:
    final_resultados = []
    for (competencia, directriz, indice, parrafo_clave), data in grouped_resultados.items():

        parrafos_resaltados= []
        palabras_a_resaltar= list(data['Palabras Clave'])

        for parrafo in sorted(list(data['P√°rrafos Encontrados'])):
            contenido_resaltado= parrafo
            for palabra in palabras_a_resaltar:
                regex= re.compile(rf'(\b{re.escape(palabra)}\b)', re.IGNORECASE)
                contenido_resaltado= regex.sub(r'**\1**', contenido_resaltado)

            parrafos_resaltados.append(contenido_resaltado)

        parrafos_final= "\n\n".join(parrafos_resaltados)

        final_resultados.append({
            'Archivo': data['Archivo'],
            'Competencia': competencia,
            'Indice': directriz,
            'SubIndice': indice,
            'Palabras Clave': ", ".join(sorted(palabras_a_resaltar)),
            'P√°rrafos Encontrados': parrafos_final,
            'Lineamiento o Directriz': data['Lineamientos'],
            'Art√≠culo del Acuerdo': ", ".join(sorted(list(data['Articulos del Acuerdo']))) if data['Articulos del Acuerdo'] else "No aplica"
        })

    df = pd.DataFrame(final_resultados)
    columnas_ord = [
        'Archivo', 'Competencia', 'Indice',
        'SubIndice', 'Palabras Clave', 'P√°rrafos Encontrados', 'Lineamiento o Directriz', 'Art√≠culo del Acuerdo'
    ]
    df = df[columnas_ord]

    df_ranking, subindices_frecuentes = generar_ranking_subindices(df)
    df_subindices_frecuentes = generar_hoja_subindices_frecuentes(df, subindices_frecuentes)


    resultados_dir = os.path.join(output_dir, "Resultados")
    os.makedirs(resultados_dir, exist_ok=True)

    if usar_filtro:
        nombre_base = "Resultados_Filtrado_por_Titulo.xlsx"

    else:


        nombre_base = "Resultados_Sin_Filtro_por_Titulo.xlsx"


    output_file = os.path.join(resultados_dir, nombre_base)
    keywords_in_title_list = []




    for file, keywords in keywords_by_file.items():
        for kw in keywords:
            keywords_in_title_list.append({
                'Archivo': file,
                'Competencia': kw['competencia'],
                'Directrices': kw['Directrices'],
                'Indice': kw['sub_idc'],
                'Palabra Clave Encontrada en el T√≠tulo': kw['clave']
            })
    df_titles = pd.DataFrame(keywords_in_title_list)
    def clasificar_competencia(competencia):
        if str(competencia).strip().upper().startswith("DSI"):
            return "Directriz"
        else:
            return "Lineamiento"
    df_titles = df_titles.assign(Clasificacion=df_titles["Competencia"].apply(clasificar_competencia))
    columnas_orden_titulos = [
        'Archivo',
        "Clasificacion",
        'Competencia',
        'Directrices',
        'Indice',
        'Palabra Clave Encontrada en el T√≠tulo'
    ]
    df_titles = df_titles[columnas_orden_titulos]
    final_articulos_agrupados = []
    for key, value in articulos_agrupados.items():
        final_articulos_agrupados.append({
            'Archivo': value['Archivo'],
            'Palabras Clave': ", ".join(sorted(list(value['Palabras Clave']))),
            'Articulo Completo': value['Articulo Completo']
        })
    df_articulos_completos = pd.DataFrame(final_articulos_agrupados)

    try:
        df_dsi = df[df['Competencia'].str.startswith('DSI')]
        df_Directrices = df[~df['Competencia'].str.startswith('DSI')]

        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
            if usar_filtro and not df_titles.empty:
                df_titles.to_excel(writer, sheet_name='Palabras Clave en T√≠tulos', index=False)
            if not df_dsi.empty:
                df_dsi.to_excel(writer, sheet_name='DIRECTRICES', index=False)
            if not df_Directrices.empty:
                df_Directrices.to_excel(writer, sheet_name='LINEAMIENTOS', index=False)
            if not df_articulos_completos.empty:
                df_articulos_completos.to_excel(writer, sheet_name='ARTICULOS COMPLETOS', index=False)
            if not df_ranking.empty:
                df_ranking.to_excel(writer, sheet_name='RANKING SUBINDICES', index=False)
            if not df_subindices_frecuentes.empty:
                df_subindices_frecuentes.to_excel(writer, sheet_name='SUBINDICES FRECUENTES', index=False)

        print(f"\nResultados del an√°lisis guardados en: {output_file}")
        print(f"Archivos de texto disponibles en: {txt_dir}")
    except Exception as e:
        print(f"\nError al guardar el archivo de Excel: {e}")